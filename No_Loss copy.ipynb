{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.CustomHybridLoss(bce_weight=0.4, mse_weight=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm, trange\n",
    "import utils\n",
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube_len = 32\n",
    "epoch_count = 400\n",
    "batch_size = 128\n",
    "noise_size = 200\n",
    "d_lr = 0.00005 # discriminator learning rate\n",
    "g_lr = 0.0025 # generator learning rate\n",
    "log_folder = \"logs/\"\n",
    "\n",
    "condition_count = 2  #taking 2 angles in consideration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "all_models1 = np.load('model_data/03636649/model_array.npy')\n",
    "all_models7 = np.load('model_data/03636649/model_array90.npy')\n",
    "\n",
    "train_set1 = torch.from_numpy(all_models1).float()\n",
    "train_set7 = torch.from_numpy(all_models7).float()\n",
    "\n",
    "train_set_all = TensorDataset(train_set1, train_set7)\n",
    "train_loader = DataLoader(dataset=train_set_all, batch_size=batch_size, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "generator = models.Generator(noise_size=(noise_size + 1), cube_resolution=cube_len) # noise size +1 condition value\n",
    "discriminator = models.Discriminator(cube_resolution=cube_len)\n",
    "\n",
    "generator = generator.to(device)\n",
    "discriminator = discriminator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizerD = optim.RMSprop(discriminator.parameters(), lr=d_lr, )\n",
    "optimizerG = optim.RMSprop(generator.parameters(), lr=g_lr, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomHybridLoss(torch.nn.Module):\n",
    "    def __init__(self, bce_weight=0.5, mse_weight=0.5):\n",
    "        super(CustomHybridLoss, self).__init__()\n",
    "        self.bce_weight = bce_weight\n",
    "        self.mse_weight = mse_weight\n",
    "        self.bce_loss = torch.nn.BCELoss()\n",
    "        self.mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        bce = self.bce_loss(predictions, targets)\n",
    "        mse = self.mse_loss(predictions, targets)\n",
    "        return self.bce_weight * bce + self.mse_weight * mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "criterion_GAN = CustomHybridLoss(bce_weight=0.6, mse_weight=0.4)\n",
    "\n",
    "def get_gan_loss(tensor,ones):\n",
    "    if(ones):\n",
    "        return criterion_GAN(tensor,Variable(torch.ones_like(tensor.data).to(device), requires_grad=False))\n",
    "    else:\n",
    "        return criterion_GAN(tensor,Variable(torch.zeros_like(tensor.data).to(device), requires_grad=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise(b_size = batch_size):\n",
    "    return torch.randn([b_size,noise_size], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_GAN_epoch():\n",
    "    \n",
    "    g_loss = []\n",
    "    d_loss = []\n",
    "    gen_out = []\n",
    "    train_disc = True\n",
    "    \n",
    "    for i, data_c in enumerate(train_loader):\n",
    "        \n",
    "        acc_list = []\n",
    "        \n",
    "        for c in range(condition_count): # train GAN for each condition\n",
    "            \n",
    "            data =  data_c[c].to(device)\n",
    "\n",
    "            discriminator.zero_grad()\n",
    "            Dr_output = discriminator(data, c)\n",
    "            errD_real = get_gan_loss(Dr_output,True)\n",
    "            \n",
    "            fake = generator(get_noise(data.shape[0]), c)\n",
    "            Df_output = discriminator(fake.detach(), c)\n",
    "            errD_fake = get_gan_loss(Df_output,False)\n",
    "\n",
    "            errD = errD_real + errD_fake\n",
    "                \n",
    "            acc_r = Dr_output.mean().item() \n",
    "            acc_f = 1.0 - Df_output.mean().item() \n",
    "            acc = (acc_r + acc_f) / 2.0\n",
    "            \n",
    "            acc_list.append(acc) # calculate discriminator accuracy\n",
    "            \n",
    "            if (train_disc): # train discriminator if the last batch accuracy is less than 0.95\n",
    "                errD.backward()\n",
    "                optimizerD.step()\n",
    "\n",
    "            generator.zero_grad() # train generator\n",
    "            fake = generator(get_noise(), c)\n",
    "            DGF_output = discriminator(fake, c)\n",
    "            errG = get_gan_loss(DGF_output,True)\n",
    "            errG.backward()\n",
    "            optimizerG.step()\n",
    "            \n",
    "            d_loss.append(errD.mean().item())\n",
    "            g_loss.append(errG.mean().item())\n",
    "\n",
    "        generator.zero_grad() # train generator for combined loss\n",
    "        discriminator.zero_grad()\n",
    "        \n",
    "        fix_noise = get_noise()\n",
    "\n",
    "        fake0 = generator(fix_noise, 0) # generate for condition 0 and 1\n",
    "        fake1 = generator(fix_noise, 1)\n",
    "        \n",
    "        fake1_rot = torch.rot90(fake1, 2) # rotate condition 1\n",
    "        fake_combined = (fake0 + fake1_rot) / 2.0 # combine them by averaging\n",
    "         \n",
    "        DGF_output_c = discriminator(fake_combined, 0) # train generator for combined output\n",
    "        errG_c = get_gan_loss(DGF_output_c,True)\n",
    "        errG_c.backward()\n",
    "        optimizerG.step()\n",
    "\n",
    "        train_disc = np.mean(acc_list) < 0.95 # decide for the next batch\n",
    "    \n",
    "    gen_out.append( fake0.detach().cpu() ) # return generated samples for condition 0, 1 and combined\n",
    "    gen_out.append( fake1.detach().cpu() )\n",
    "    gen_out.append( fake_combined.detach().cpu() )\n",
    "    \n",
    "    return np.mean(d_loss), np.mean(g_loss) , gen_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.clear_folder(log_folder) # create log folder\n",
    "log_file = open(log_folder +\"logs.txt\" ,\"a\") # open log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 --> d_loss:0.996 g_loss:2.758 time:2.117:   0%|          | 0/401 [03:47<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory models2 does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(i\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m40\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m): \u001b[38;5;66;03m# save generated samples for each 10th epoch because it takes a long time to visualize the samples\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     utils\u001b[38;5;241m.\u001b[39mvisualize_all(gen, save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, name \u001b[38;5;241m=\u001b[39m log_folder \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i))\n\u001b[1;32m---> 26\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodels2/CustomHybridLossBM_model_state_dict_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m        \n",
      "File \u001b[1;32md:\\programs\\Anaconda\\envs\\Shikai\\Lib\\site-packages\\torch\\serialization.py:627\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    624\u001b[0m _check_save_filelike(f)\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 627\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    628\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[0;32m    629\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32md:\\programs\\Anaconda\\envs\\Shikai\\Lib\\site-packages\\torch\\serialization.py:501\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[1;34m(name_or_buffer)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[1;32m--> 501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\programs\\Anaconda\\envs\\Shikai\\Lib\\site-packages\\torch\\serialization.py:472\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream))\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 472\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Parent directory models2 does not exist."
     ]
    }
   ],
   "source": [
    "d_list = []\n",
    "g_list = []\n",
    "\n",
    "pbar = tqdm( range(epoch_count+1) )\n",
    "for i in pbar :\n",
    "    \n",
    "    startTime = time.time()\n",
    "    \n",
    "    d_loss, g_loss, gen = train_GAN_epoch() #train GAN for 1 epoch\n",
    "    \n",
    "    d_list.append(d_loss) # get discriminator and generator loss\n",
    "    g_list.append(g_loss)\n",
    "    \n",
    "    utils.plot_graph([d_list,g_list], log_folder + \"loss_graph\") # plot loss graph up to that epoch\n",
    "\n",
    "    epoch_time = time.time() - startTime\n",
    "    \n",
    "    writeString = \"epoch %d --> d_loss:%0.3f g_loss:%0.3f time:%0.3f\" % (i, d_loss, g_loss, epoch_time) # generate log string\n",
    "\n",
    "    pbar.set_description(writeString)\n",
    "    log_file.write(writeString + \"\\n\") # write to log file\n",
    "    log_file.flush()\n",
    "    \n",
    "    if(i%40 == 0): # save generated samples for each 10th epoch because it takes a long time to visualize the samples\n",
    "        utils.visualize_all(gen, save=True, name = log_folder + \"samples_epoch\" + str(i))\n",
    "        torch.save(generator.state_dict(), f'models2/CustomHybridLossBM_model_state_dict_{i}.pth')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.CustomHybridLoss(bce_weight=0.4, mse_weight=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 400 --> d_loss:0.093 g_loss:1.389 time:2.680: 100%|██████████| 401/401 [44:06<00:00,  6.60s/it]  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm, trange\n",
    "import utils\n",
    "import models\n",
    "\n",
    "cube_len = 32\n",
    "epoch_count = 400\n",
    "batch_size = 128\n",
    "noise_size = 200\n",
    "d_lr = 0.00005 # discriminator learning rate\n",
    "g_lr = 0.0025 # generator learning rate\n",
    "log_folder = \"CustomHybridLoss/\"\n",
    "\n",
    "condition_count = 2\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "all_models1 = np.load('model_data/03636649/model_array.npy')\n",
    "all_models7 = np.load('model_data/03636649/model_array90.npy')\n",
    "\n",
    "train_set1 = torch.from_numpy(all_models1).float()\n",
    "train_set7 = torch.from_numpy(all_models7).float()\n",
    "\n",
    "train_set_all = TensorDataset(train_set1, train_set7)\n",
    "train_loader = DataLoader(dataset=train_set_all, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "generator = models.Generator(noise_size=(noise_size + 1), cube_resolution=cube_len) # noise size +1 condition value\n",
    "discriminator = models.Discriminator(cube_resolution=cube_len)\n",
    "\n",
    "generator = generator.to(device)\n",
    "discriminator = discriminator.to(device)\n",
    "\n",
    "optimizerD = optim.RMSprop(discriminator.parameters(), lr=d_lr, )\n",
    "optimizerG = optim.RMSprop(generator.parameters(), lr=g_lr, )\n",
    "\n",
    "class CustomHybridLoss(torch.nn.Module):\n",
    "    def __init__(self, bce_weight=0.5, mse_weight=0.5):\n",
    "        super(CustomHybridLoss, self).__init__()\n",
    "        self.bce_weight = bce_weight\n",
    "        self.mse_weight = mse_weight\n",
    "        self.bce_loss = torch.nn.BCELoss()\n",
    "        self.mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        bce = self.bce_loss(predictions, targets)\n",
    "        mse = self.mse_loss(predictions, targets)\n",
    "        return self.bce_weight * bce + self.mse_weight * mse\n",
    "\n",
    "from torch.autograd import Variable\n",
    "criterion_GAN = CustomHybridLoss(bce_weight=0.4, mse_weight=0.6)\n",
    "\n",
    "def get_gan_loss(tensor,ones):\n",
    "    if(ones):\n",
    "        return criterion_GAN(tensor,Variable(torch.ones_like(tensor.data).to(device), requires_grad=False))\n",
    "    else:\n",
    "        return criterion_GAN(tensor,Variable(torch.zeros_like(tensor.data).to(device), requires_grad=False))\n",
    "\n",
    "def get_noise(b_size = batch_size):\n",
    "    return torch.randn([b_size,noise_size], device=device)\n",
    "\n",
    "print(device)\n",
    "\n",
    "def train_GAN_epoch():\n",
    "\n",
    "    g_loss = []\n",
    "    d_loss = []\n",
    "    gen_out = []\n",
    "    train_disc = True\n",
    "\n",
    "    for i, data_c in enumerate(train_loader):\n",
    "\n",
    "        acc_list = []\n",
    "\n",
    "        for c in range(condition_count): # train GAN for each condition\n",
    "\n",
    "            data =  data_c[c].to(device)\n",
    "\n",
    "            discriminator.zero_grad()\n",
    "            Dr_output = discriminator(data, c)\n",
    "            errD_real = get_gan_loss(Dr_output,True)\n",
    "\n",
    "            fake = generator(get_noise(data.shape[0]), c)\n",
    "            Df_output = discriminator(fake.detach(), c)\n",
    "            errD_fake = get_gan_loss(Df_output,False)\n",
    "\n",
    "            errD = errD_real + errD_fake\n",
    "\n",
    "            acc_r = Dr_output.mean().item()\n",
    "            acc_f = 1.0 - Df_output.mean().item()\n",
    "            acc = (acc_r + acc_f) / 2.0\n",
    "\n",
    "            acc_list.append(acc) # calculate discriminator accuracy\n",
    "\n",
    "            if (train_disc): # train discriminator if the last batch accuracy is less than 0.95\n",
    "                errD.backward()\n",
    "                optimizerD.step()\n",
    "\n",
    "            generator.zero_grad() # train generator\n",
    "            fake = generator(get_noise(), c)\n",
    "            DGF_output = discriminator(fake, c)\n",
    "            errG = get_gan_loss(DGF_output,True)\n",
    "            errG.backward()\n",
    "            optimizerG.step()\n",
    "\n",
    "            d_loss.append(errD.mean().item())\n",
    "            g_loss.append(errG.mean().item())\n",
    "\n",
    "        generator.zero_grad() # train generator for combined loss\n",
    "        discriminator.zero_grad()\n",
    "\n",
    "        fix_noise = get_noise()\n",
    "\n",
    "        fake0 = generator(fix_noise, 0) # generate for condition 0 and 1\n",
    "        fake1 = generator(fix_noise, 1)\n",
    "\n",
    "        fake1_rot = torch.rot90(fake1, 2) # rotate condition 1\n",
    "        fake_combined = (fake0 + fake1_rot) / 2.0 # combine them by averaging\n",
    "\n",
    "        DGF_output_c = discriminator(fake_combined, 0) # train generator for combined output\n",
    "        errG_c = get_gan_loss(DGF_output_c,True)\n",
    "        errG_c.backward()\n",
    "        optimizerG.step()\n",
    "\n",
    "        train_disc = np.mean(acc_list) < 0.95 # decide for the next batch\n",
    "\n",
    "    gen_out.append( fake0.detach().cpu() ) # return generated samples for condition 0, 1 and combined\n",
    "    gen_out.append( fake1.detach().cpu() )\n",
    "    gen_out.append( fake_combined.detach().cpu() )\n",
    "\n",
    "    return np.mean(d_loss), np.mean(g_loss) , gen_out\n",
    "\n",
    "utils.clear_folder(log_folder) # create log folder\n",
    "log_file = open(log_folder +\"logs.txt\" ,\"a\") # open log file\n",
    "\n",
    "d_list = []\n",
    "g_list = []\n",
    "\n",
    "pbar = tqdm( range(epoch_count+1) )\n",
    "for i in pbar :\n",
    "\n",
    "    startTime = time.time()\n",
    "\n",
    "    d_loss, g_loss, gen = train_GAN_epoch() #train GAN for 1 epoch\n",
    "\n",
    "    d_list.append(d_loss) # get discriminator and generator loss\n",
    "    g_list.append(g_loss)\n",
    "\n",
    "    utils.plot_graph([d_list,g_list], log_folder + \"loss_graph\") # plot loss graph up to that epoch\n",
    "\n",
    "    epoch_time = time.time() - startTime\n",
    "\n",
    "    writeString = \"epoch %d --> d_loss:%0.3f g_loss:%0.3f time:%0.3f\" % (i, d_loss, g_loss, epoch_time) # generate log string\n",
    "\n",
    "    pbar.set_description(writeString)\n",
    "    log_file.write(writeString + \"\\n\") # write to log file\n",
    "    log_file.flush()\n",
    "\n",
    "    if(i%40 == 0): # save generated samples for each 10th epoch because it takes a long time to visualize the samples\n",
    "        utils.visualize_all(gen, save=True, name = log_folder + \"samples_epoch\" + str(i))\n",
    "\n",
    "        torch.save(generator.state_dict(), f'models2/CustomHybridLossMB_model_state_dict_{i}.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.bce_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 400 --> d_loss:0.685 g_loss:1.842 time:6.675: 100%|██████████| 401/401 [41:46<00:00,  6.25s/it]  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm, trange\n",
    "import utils\n",
    "import models\n",
    "\n",
    "cube_len = 32\n",
    "epoch_count = 400\n",
    "batch_size = 128\n",
    "noise_size = 200\n",
    "d_lr = 0.00005 # discriminator learning rate\n",
    "g_lr = 0.0025 # generator learning rate\n",
    "log_folder = \"BCELoss_L12/\"\n",
    "\n",
    "condition_count = 2\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "all_models1 = np.load('model_data/03636649/model_array.npy')\n",
    "all_models7 = np.load('model_data/03636649/model_array90.npy')\n",
    "\n",
    "\n",
    "train_set1 = torch.from_numpy(all_models1).float()\n",
    "train_set7 = torch.from_numpy(all_models7).float()\n",
    "\n",
    "train_set_all = TensorDataset(train_set1, train_set7)\n",
    "train_loader = DataLoader(dataset=train_set_all, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "generator = models.Generator(noise_size=(noise_size + 1), cube_resolution=cube_len) # noise size +1 condition value\n",
    "discriminator = models.Discriminator(cube_resolution=cube_len)\n",
    "\n",
    "generator = generator.to(device)\n",
    "discriminator = discriminator.to(device)\n",
    "\n",
    "optimizerD = optim.RMSprop(discriminator.parameters(), lr=d_lr, )\n",
    "optimizerG = optim.RMSprop(generator.parameters(), lr=g_lr, )\n",
    "\n",
    "class CustomHybridLoss(torch.nn.Module):\n",
    "    def __init__(self, bce_weight=0.5, BceLL_weight=0.5):\n",
    "        super(CustomHybridLoss, self).__init__()\n",
    "        self.bce_weight = bce_weight\n",
    "        self.BceLL_weight = BceLL_weight\n",
    "        self.bce_loss = torch.nn.BCELoss()\n",
    "        self.BCEWithLogitsLoss = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, predictions, targets, ):\n",
    "        bce = self.bce_loss(predictions, targets)\n",
    "        l1 = self.BCEWithLogitsLoss(predictions, targets)\n",
    "        return self.bce_weight * bce + self.BceLL_weight * l1\n",
    "\n",
    "\n",
    "from torch.autograd import Variable\n",
    "criterion_GAN = CustomHybridLoss(bce_weight=0.7, BceLL_weight=0.3)\n",
    "\n",
    "def get_gan_loss(tensor,ones):\n",
    "    if(ones):\n",
    "        return criterion_GAN(tensor,Variable(torch.ones_like(tensor.data).to(device), requires_grad=False))\n",
    "    else:\n",
    "        return criterion_GAN(tensor,Variable(torch.zeros_like(tensor.data).to(device), requires_grad=False))\n",
    "\n",
    "def get_noise(b_size = batch_size):\n",
    "    return torch.randn([b_size,noise_size], device=device)\n",
    "\n",
    "print(device)\n",
    "\n",
    "def train_GAN_epoch():\n",
    "\n",
    "    g_loss = []\n",
    "    d_loss = []\n",
    "    gen_out = []\n",
    "    train_disc = True\n",
    "\n",
    "    for i, data_c in enumerate(train_loader):\n",
    "\n",
    "        acc_list = []\n",
    "\n",
    "        for c in range(condition_count): # train GAN for each condition\n",
    "\n",
    "            data =  data_c[c].to(device)\n",
    "\n",
    "            discriminator.zero_grad()\n",
    "            Dr_output = discriminator(data, c)\n",
    "            errD_real = get_gan_loss(Dr_output,True)\n",
    "\n",
    "            fake = generator(get_noise(data.shape[0]), c)\n",
    "            Df_output = discriminator(fake.detach(), c)\n",
    "            errD_fake = get_gan_loss(Df_output,False)\n",
    "\n",
    "            errD = errD_real + errD_fake\n",
    "\n",
    "            acc_r = Dr_output.mean().item()\n",
    "            acc_f = 1.0 - Df_output.mean().item()\n",
    "            acc = (acc_r + acc_f) / 2.0\n",
    "\n",
    "            acc_list.append(acc) # calculate discriminator accuracy\n",
    "\n",
    "            if (train_disc): # train discriminator if the last batch accuracy is less than 0.95\n",
    "                errD.backward()\n",
    "                optimizerD.step()\n",
    "\n",
    "            generator.zero_grad() # train generator\n",
    "            fake = generator(get_noise(), c)\n",
    "            DGF_output = discriminator(fake, c)\n",
    "            errG = get_gan_loss(DGF_output,True)\n",
    "            errG.backward()\n",
    "            optimizerG.step()\n",
    "\n",
    "            d_loss.append(errD.mean().item())\n",
    "            g_loss.append(errG.mean().item())\n",
    "\n",
    "        generator.zero_grad() # train generator for combined loss\n",
    "        discriminator.zero_grad()\n",
    "\n",
    "        fix_noise = get_noise()\n",
    "\n",
    "        fake0 = generator(fix_noise, 0) # generate for condition 0 and 1\n",
    "        fake1 = generator(fix_noise, 1)\n",
    "\n",
    "        fake1_rot = torch.rot90(fake1, 2) # rotate condition 1\n",
    "        fake_combined = (fake0 + fake1_rot) / 2.0 # combine them by averaging\n",
    "\n",
    "        DGF_output_c = discriminator(fake_combined, 0) # train generator for combined output\n",
    "        errG_c = get_gan_loss(DGF_output_c,True)\n",
    "        errG_c.backward()\n",
    "        optimizerG.step()\n",
    "\n",
    "        train_disc = np.mean(acc_list) < 0.95 # decide for the next batch\n",
    "\n",
    "    gen_out.append( fake0.detach().cpu() ) # return generated samples for condition 0, 1 and combined\n",
    "    gen_out.append( fake1.detach().cpu() )\n",
    "    gen_out.append( fake_combined.detach().cpu() )\n",
    "\n",
    "    return np.mean(d_loss), np.mean(g_loss) , gen_out\n",
    "\n",
    "utils.clear_folder(log_folder) # create log folder\n",
    "log_file = open(log_folder +\"logs.txt\" ,\"a\") # open log file\n",
    "\n",
    "d_list = []\n",
    "g_list = []\n",
    "\n",
    "pbar = tqdm( range(epoch_count+1) )\n",
    "for i in pbar :\n",
    "\n",
    "    startTime = time.time()\n",
    "\n",
    "    d_loss, g_loss, gen = train_GAN_epoch() #train GAN for 1 epoch\n",
    "\n",
    "    d_list.append(d_loss) # get discriminator and generator loss\n",
    "    g_list.append(g_loss)\n",
    "\n",
    "    utils.plot_graph([d_list,g_list], log_folder + \"loss_graph\") # plot loss graph up to that epoch\n",
    "\n",
    "    epoch_time = time.time() - startTime\n",
    "\n",
    "    writeString = \"epoch %d --> d_loss:%0.3f g_loss:%0.3f time:%0.3f\" % (i, d_loss, g_loss, epoch_time) # generate log string\n",
    "\n",
    "    pbar.set_description(writeString)\n",
    "    log_file.write(writeString + \"\\n\") # write to log file\n",
    "    log_file.flush()\n",
    "\n",
    "    if(i%40 == 0): # save generated samples for each 10th epoch because it takes a long time to visualize the samples\n",
    "        utils.visualize_all(gen, save=True, name = log_folder + \"samples_epoch\" + str(i))\n",
    "        torch.save(generator.state_dict(), f'models2/BCE_L1_model_state_dict_{i}.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 400 --> d_loss:0.482 g_loss:1.780 time:2.257: 100%|██████████| 401/401 [31:10<00:00,  4.67s/it]  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm, trange\n",
    "import utils\n",
    "import models\n",
    "\n",
    "cube_len = 32\n",
    "epoch_count = 400\n",
    "batch_size = 128\n",
    "noise_size = 200\n",
    "d_lr = 0.00005 # discriminator learning rate\n",
    "g_lr = 0.0025 # generator learning rate\n",
    "log_folder = \"TRIPLElOSS/\"\n",
    "\n",
    "condition_count = 2\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "all_models1 = np.load('model_data/03636649/model_array.npy')\n",
    "all_models7 = np.load('model_data/03636649/model_array90.npy')\n",
    "\n",
    "\n",
    "train_set1 = torch.from_numpy(all_models1).float()\n",
    "train_set7 = torch.from_numpy(all_models7).float()\n",
    "\n",
    "train_set_all = TensorDataset(train_set1, train_set7)\n",
    "train_loader = DataLoader(dataset=train_set_all, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "generator = models.Generator(noise_size=(noise_size + 1), cube_resolution=cube_len) # noise size +1 condition value\n",
    "discriminator = models.Discriminator(cube_resolution=cube_len)\n",
    "\n",
    "generator = generator.to(device)\n",
    "discriminator = discriminator.to(device)\n",
    "\n",
    "optimizerD = optim.RMSprop(discriminator.parameters(), lr=d_lr, )\n",
    "optimizerG = optim.RMSprop(generator.parameters(), lr=g_lr, )\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Custom3DGANLoss(nn.Module):\n",
    "    def __init__(self, bce_weight=0.4, l1_weight=0.3, voxel_weight=0.3):\n",
    "        super(Custom3DGANLoss, self).__init__()\n",
    "        self.bce_weight = bce_weight\n",
    "        self.l1_weight = l1_weight\n",
    "        self.voxel_weight = voxel_weight\n",
    "        \n",
    "        self.bce_loss = nn.BCELoss()\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "    \n",
    "    def voxel_loss(self, predictions, targets):\n",
    "\n",
    "        occupied_loss = F.binary_cross_entropy_with_logits(predictions, targets)\n",
    "\n",
    "        consistency_loss = F.mse_loss(torch.sigmoid(predictions), targets)\n",
    "        \n",
    "        return occupied_loss + consistency_loss\n",
    "    \n",
    "    def forward(self, predictions, targets):\n",
    "        bce = self.bce_loss(predictions, targets)\n",
    "        l1 = self.l1_loss(predictions, targets)\n",
    "        voxel = self.voxel_loss(predictions, targets)\n",
    "        \n",
    "        total_loss = (self.bce_weight * bce + \n",
    "                      self.l1_weight * l1 + \n",
    "                      self.voxel_weight * voxel)\n",
    "        \n",
    "        return total_loss\n",
    "from torch.autograd import Variable\n",
    "criterion_GAN = Custom3DGANLoss(bce_weight=0.4, l1_weight=0.3, voxel_weight=0.3)\n",
    "\n",
    "def get_gan_loss(tensor,ones):\n",
    "    if(ones):\n",
    "        return criterion_GAN(tensor,Variable(torch.ones_like(tensor.data).to(device), requires_grad=False))\n",
    "    else:\n",
    "        return criterion_GAN(tensor,Variable(torch.zeros_like(tensor.data).to(device), requires_grad=False))\n",
    "\n",
    "def get_noise(b_size = batch_size):\n",
    "    return torch.randn([b_size,noise_size], device=device)\n",
    "\n",
    "print(device)\n",
    "\n",
    "def train_GAN_epoch():\n",
    "\n",
    "    g_loss = []\n",
    "    d_loss = []\n",
    "    gen_out = []\n",
    "    train_disc = True\n",
    "\n",
    "    for i, data_c in enumerate(train_loader):\n",
    "\n",
    "        acc_list = []\n",
    "\n",
    "        for c in range(condition_count):\n",
    "\n",
    "            data =  data_c[c].to(device)\n",
    "\n",
    "            discriminator.zero_grad()\n",
    "            Dr_output = discriminator(data, c)\n",
    "            errD_real = get_gan_loss(Dr_output,True)\n",
    "\n",
    "            fake = generator(get_noise(data.shape[0]), c)\n",
    "            Df_output = discriminator(fake.detach(), c)\n",
    "            errD_fake = get_gan_loss(Df_output,False)\n",
    "\n",
    "            errD = errD_real + errD_fake\n",
    "\n",
    "            acc_r = Dr_output.mean().item()\n",
    "            acc_f = 1.0 - Df_output.mean().item()\n",
    "            acc = (acc_r + acc_f) / 2.0\n",
    "\n",
    "            acc_list.append(acc) \n",
    "\n",
    "            if (train_disc): \n",
    "                errD.backward()\n",
    "                optimizerD.step()\n",
    "\n",
    "            generator.zero_grad() # train generator\n",
    "            fake = generator(get_noise(), c)\n",
    "            DGF_output = discriminator(fake, c)\n",
    "            errG = get_gan_loss(DGF_output,True)\n",
    "            errG.backward()\n",
    "            optimizerG.step()\n",
    "\n",
    "            d_loss.append(errD.mean().item())\n",
    "            g_loss.append(errG.mean().item())\n",
    "\n",
    "        generator.zero_grad() # train generator for combined loss\n",
    "        discriminator.zero_grad()\n",
    "\n",
    "        fix_noise = get_noise()\n",
    "\n",
    "        fake0 = generator(fix_noise, 0) # generate for condition 0 and 1\n",
    "        fake1 = generator(fix_noise, 1)\n",
    "\n",
    "        fake1_rot = torch.rot90(fake1, 2) # rotate condition 1\n",
    "        fake_combined = (fake0 + fake1_rot) / 2.0 # combine them by averaging\n",
    "\n",
    "        DGF_output_c = discriminator(fake_combined, 0) # train generator for combined output\n",
    "        errG_c = get_gan_loss(DGF_output_c,True)\n",
    "        errG_c.backward()\n",
    "        optimizerG.step()\n",
    "\n",
    "        train_disc = np.mean(acc_list) < 0.95 # decide for the next batch\n",
    "\n",
    "    gen_out.append( fake0.detach().cpu() ) # return generated samples for condition 0, 1 and combined\n",
    "    gen_out.append( fake1.detach().cpu() )\n",
    "    gen_out.append( fake_combined.detach().cpu() )\n",
    "\n",
    "    return np.mean(d_loss), np.mean(g_loss) , gen_out\n",
    "\n",
    "utils.clear_folder(log_folder) # create log folder\n",
    "log_file = open(log_folder +\"logs.txt\" ,\"a\") # open log file\n",
    "\n",
    "d_list = []\n",
    "g_list = []\n",
    "\n",
    "pbar = tqdm( range(epoch_count+1) )\n",
    "for i in pbar :\n",
    "\n",
    "    startTime = time.time()\n",
    "\n",
    "    d_loss, g_loss, gen = train_GAN_epoch() #train GAN for 1 epoch\n",
    "\n",
    "    d_list.append(d_loss) # get discriminator and generator loss\n",
    "    g_list.append(g_loss)\n",
    "\n",
    "    utils.plot_graph([d_list,g_list], log_folder + \"loss_graph\") # plot loss graph up to that epoch\n",
    "\n",
    "    epoch_time = time.time() - startTime\n",
    "\n",
    "    writeString = \"epoch %d --> d_loss:%0.3f g_loss:%0.3f time:%0.3f\" % (i, d_loss, g_loss, epoch_time) # generate log string\n",
    "\n",
    "    pbar.set_description(writeString)\n",
    "    log_file.write(writeString + \"\\n\") # write to log file\n",
    "    log_file.flush()\n",
    "\n",
    "    if(i%40 == 0): # save generated samples for each 10th epoch because it takes a long time to visualize the samples\n",
    "        utils.visualize_all(gen, save=True, name = log_folder + \"samples_epoch\" + str(i))\n",
    "        torch.save(generator.state_dict(), f'models2/Triple_Loss_model_state_dict_{i}.pth')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Shikai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
